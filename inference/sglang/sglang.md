## ðŸ” RediX Attention & Prefix Trie: Smarter Inference with Shared Prompts

Modern LLM serving suffers from one key inefficiency: **prompt overlap isnâ€™t reused**. If thousands of users start their queries with the same few words, we end up re-computing and caching the same prefix again and again. Even with PagedAttention, this problem remains.

RediX Attention, introduced in the [sgLang](https://arxiv.org/pdf/2312.07104) paper (2024), solves this by organizing prompts into a **prefix tree (trie)**. Prompts that share common prefixes reuse computation for shared partsâ€”just like a compiler deduplicates shared code.

---

### ðŸŒ² Prefix Tree

Letâ€™s say we have these prompts:

- `"Tell me about transformers"`
- `"Tell me about diffusion models"`
- `"Tell me about sgLang"`

Instead of computing KV for each token per prompt, we build a **prefix tree** like this:

```
"Tell"
â”‚
"me"
â”‚
"about"
    â”œâ”€â”€ "transformers"
    â”œâ”€â”€ "diffusion"
    â””â”€â”€ "sgLang"
```


Now:
- The KV cache for `"Tell me about"` is computed once
- Only `"transformers"`, `"diffusion"`, and `"sgLang"` need separate suffix computation

---

### ðŸ’¡ RediX Attention: Key Concepts

| Concept             | Role                                                                 |
|---------------------|----------------------------------------------------------------------|
| **Prefix Trie**     | Organizes shared tokens across batch prompts                         |
| **Node KV Cache**   | Stores partial KV for each unique prefix                             |
| **RediX Execution** | Decodes tokens by traversing the trie and caching at each node       |
| **Pointer Mask**    | Maps generated tokens back to their original prompt lineage          |

---

### âš™ï¸ Implementation Pseudocode

```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.kv_cache = None  # KV for this node

def insert(trie, prompt_tokens):
    node = trie
    for tok in prompt_tokens:
        if tok not in node.children:
            node.children[tok] = TrieNode()
        node = node.children[tok]

def decode_with_redix(trie_root):
    # Traverse and decode each node only once
    for token, child in trie_root.children.items():
        if not child.kv_cache:
            child.kv_cache = run_attention(token, parent_kv=trie_root.kv_cache)
        decode_with_redix(child)
```
## ðŸ“ˆ Results from the Paper
Metric	RediX Attention	vLLM (PagedAttention)
Throughput (â†‘ better)	5x	1x
Memory usage (â†“ better)	-30%	Baseline
Latency for long shared prompts	Reduced	High

RediX particularly shines in:

RAG pipelines (same instructions, different contexts)

Chatbots (template-driven queries)

Agentic workflows (shared control flow)

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       Traditional Decoding         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
Prompt 1: [Tell, me, about, transformers]
Prompt 2: [Tell, me, about, diffusion]
Prompt 3: [Tell, me, about, sgLang]
```

- KV computed for all tokens in all 3 prompts, Total: 15 tokens' worth of KV

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚          RediX Attention           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”Œâ”€â”€ Tell
             â”‚   â””â”€â”€ me
             â”‚       â””â”€â”€ about
             â”‚           â”œâ”€â”€ transformers
             â”‚           â”œâ”€â”€ diffusion
             â”‚           â””â”€â”€ sgLang

- KV computed once for 'Tell me about'

- Only compute suffixes separately

- Saves memory + boosts speed


### ðŸ§ª Some Applications of RediX Attention

RediX Attention shines in any setting where many user prompts share a **common structure**, such as:

- ðŸ“š **RAG Pipelines**: Same instruction template with different retrieved context
- ðŸ¤– **Chatbots & Assistants**: Common greeting or instruction followed by user-specific input
- ðŸ” **Search + Generation Systems**: â€œSummarize this document: â€¦â€ across many documents
- ðŸ§  **LLM Agents & Tools**: Repeated control instructions with changing parameters
- ðŸ§ª **Few-Shot Learning**: Shared in-context examples with different test prompts
---

#### ðŸ§µ Example: RAG with Shared Prompt Template

Letâ€™s say weâ€™re building a document Q&A system.

Each prompt has this format:

> "Answer the question based on the following context: 
[CONTEXT] Question: [QUESTION]"

If we receive 1,000 user queries simultaneously, and all use the same template, RediX Attention optimizes the computation by reusing the shared prefix:

```"Answer"
â””â”€â”€ "the"
â””â”€â”€ "question"
â””â”€â”€ "based"
â””â”€â”€ "on"
â””â”€â”€ ...
â”œâ”€â”€ context: A + Q1
â”œâ”€â”€ context: B + Q2
â””â”€â”€ context: C + Q3
```


 Instead of calculating KV attention for the full sequence 1,000 times, we compute once for the shared prefix and only diverge at the context and question parts.

---

#### ðŸ§µ Few-Shot Learning with RediX

In few-shot prompting, we often prepend **in-context examples** like this:

```Prompt 1:
Q: What is the capital of France?
A: Paris
Q: What is the capital of Germany?

Prompt 2:
Q: What is the capital of France?
A: Paris
Q: What is the capital of Spain?

Prompt 3:
Q: What is the capital of France?
A: Paris
Q: What is the capital of Italy?
```


ðŸ’¡ All prompts **share the same in-context example**, and only differ at the last query. RediX constructs a trie like this:

```
Q: What is the capital of France?
â””â”€â”€ A: Paris
â””â”€â”€ Q: What is the capital of ...
â”œâ”€â”€ Germany
â”œâ”€â”€ Spain
â””â”€â”€ Italy
```



#### ðŸ§  Summary of RediX Benefits

| Use Case               | Benefit                                    |
|------------------------|---------------------------------------------|
| Few-shot learning      | Reuse of shared examples = less recompute  |
| RAG                    | Efficient prefix reuse across retrieved docs |
| Multi-agent control    | Shared scaffolding logic = cleaner KV reuse |
| Search or summarization| Common templated prompts = memory savings  |

RediX effectively exploits the **prefix redundancy** thatâ€™s inherent to many structured LLM applications, especially those using prompt engineering or templating.


RediX is especially helpful for **template-driven generation**, which is common in production LLM deployments across industries like:

- Customer support bots
- Financial document analysis
- Legal contract summarization
- Educational tutoring systems
